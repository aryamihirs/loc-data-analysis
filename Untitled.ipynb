{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787cfbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x81 in position 24519: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE1_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Update TABLE1_PATH to your file.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     t2 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTABLE2_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTABLE2_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Update TABLE2_PATH to your file.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/product/loc-data-analysis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/product/loc-data-analysis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/product/loc-data-analysis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/product/loc-data-analysis/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/product/loc-data-analysis/.venv/lib/python3.13/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:574\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:663\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._get_header\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:325\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x81 in position 24519: invalid start byte"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- PARAMETERS YOU CAN EDIT ---\n",
    "SALARY = 64.9  # your salary in the same units as the Level columns\n",
    "SOCCODE_FILTER = \"15-1299\"  # SOC code to filter for\n",
    "\n",
    "# File paths â€“ update these to your real files\n",
    "TABLE1_PATH = \"/Users/aryamihirs/projects/product/loc-data-analysis/OFLC_Wages_2023-24/ALC_Export.csv\"  # contains: Area, SocCode, Level1, Level2, Level3, Level4, Average, Label\n",
    "TABLE2_PATH = \"/Users/aryamihirs/projects/product/loc-data-analysis/OFLC_Wages_2023-24/Geography.csv\"  # contains: Area, AreaName, StateAb, State, CountyTownName\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "# If your data is in Excel instead of CSV, use pd.read_excel(\"file.xlsx\", sheet_name=\"...\") instead\n",
    "\n",
    "try:\n",
    "    # low_memory=False avoids mixed-type chunking issues and DtypeWarning\n",
    "    t1 = pd.read_csv(TABLE1_PATH, low_memory=False)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Could not find {TABLE1_PATH}. Update TABLE1_PATH to your file.\")\n",
    "\n",
    "try:\n",
    "    # Try reading with UTF-8 first, then fallback to other encodings if needed\n",
    "    try:\n",
    "        t2 = pd.read_csv(TABLE2_PATH, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Try common alternative encodings\n",
    "        for encoding in ['latin-1', 'iso-8859-1', 'cp1252', 'windows-1252']:\n",
    "            try:\n",
    "                t2 = pd.read_csv(TABLE2_PATH, encoding=encoding)\n",
    "                print(f\"Note: Geography.csv read with {encoding} encoding\")\n",
    "                break\n",
    "            except (UnicodeDecodeError, LookupError):\n",
    "                continue\n",
    "        else:\n",
    "            # If all encodings fail, try with errors='ignore' or 'replace'\n",
    "            t2 = pd.read_csv(TABLE2_PATH, encoding='utf-8', errors='replace')\n",
    "            print(\"Warning: Geography.csv read with UTF-8 encoding and error replacement\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Could not find {TABLE2_PATH}. Update TABLE2_PATH to your file.\")\n",
    "\n",
    "# --- ENSURE NUMERIC LEVEL COLUMNS ---\n",
    "# Adjust this list if your level column names differ\n",
    "for col in [\"Level1\", \"Level2\", \"Level3\", \"Level4\", \"Average\"]:\n",
    "    if col in t1.columns:\n",
    "        # Clean out currency symbols, commas, and any non-numeric chars before conversion\n",
    "        cleaned = (\n",
    "            t1[col]\n",
    "            .astype(str)\n",
    "            .str.replace(r\"[^0-9.\\-]\", \"\", regex=True)\n",
    "            .replace({\"\": pd.NA})\n",
    "        )\n",
    "        t1[col] = pd.to_numeric(cleaned, errors=\"coerce\")\n",
    "\n",
    "# --- FILTER TO ROWS WHERE SALARY IS IN LEVEL4 ---\n",
    "# This step assumes Level4 is either:\n",
    "#   (a) a single threshold (e.g., min salary for Level4), OR\n",
    "#   (b) the midpoint/target for Level4\n",
    "# If your meaning is different, we can adjust the logic.\n",
    "\n",
    "# Example 1: salary >= Level4 threshold\n",
    "eligible_mask = SALARY >= t1[\"Level4\"]\n",
    "\n",
    "# Also require the desired SOC code\n",
    "if \"SocCode\" in t1.columns:\n",
    "    eligible_mask &= t1[\"SocCode\"] == SOCCODE_FILTER\n",
    "\n",
    "# If you instead want the *closest* Level to your salary, we could change the logic.\n",
    "\n",
    "eligible_t1 = t1[eligible_mask].copy()\n",
    "\n",
    "# --- JOIN WITH LOCATION INFO ---\n",
    "# Both tables share the \"Area\" column according to your metadata.\n",
    "\n",
    "eligible_locations = eligible_t1.merge(t2, on=\"Area\", how=\"left\")\n",
    "\n",
    "# --- SELECT & DISPLAY FINAL COLUMNS ---\n",
    "# Adjust the order/columns as you like.\n",
    "final_cols = [\n",
    "    col\n",
    "    for col in [\n",
    "        \"Area\",\n",
    "        \"AreaName\",\n",
    "        \"StateAb\",\n",
    "        \"State\",\n",
    "        \"CountyTownName\",\n",
    "        \"SocCode\",\n",
    "        \"Level1\",\n",
    "        \"Level2\",\n",
    "        \"Level3\",\n",
    "        \"Level4\",\n",
    "        \"Average\",\n",
    "        \"Label\",\n",
    "    ]\n",
    "    if col in eligible_locations.columns\n",
    "]\n",
    "\n",
    "final_table = eligible_locations[final_cols].sort_values([\"StateAb\", \"AreaName\"], na_position=\"last\")\n",
    "\n",
    "print(\"Number of eligible locations:\", len(final_table))\n",
    "final_table.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27bf36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA DIAGNOSTICS ===\n",
      "\n",
      "Table 1 columns: ['Area', 'SocCode', 'GeoLvl', 'Level1', 'Level2', 'Level3', 'Level4', 'Average', 'Label']\n",
      "\n",
      "Table 1 shape: (451984, 9)\n",
      "\n",
      "First few rows of Table 1:\n",
      "    Area  SocCode  GeoLvl  Level1  Level2  Level3  Level4  Average Label\n",
      "0  10180  11-1011       1   55.83   85.46  115.10  144.73   115.39   NaN\n",
      "1  10180  11-1021       1   19.29   33.25   47.20   61.16    47.35   NaN\n",
      "2  10180  11-2021       1   30.74   43.26   55.79   68.31    55.91   NaN\n",
      "3  10180  11-2022       1   25.79   38.75   51.70   64.66    51.83   NaN\n",
      "4  10180  11-2032       1   26.99   34.83   42.68   50.52    42.76   NaN\n",
      "\n",
      "=== SOC CODE ANALYSIS ===\n",
      "\n",
      "Looking for SOC code: '15-1299'\n",
      "Unique SOC codes (first 20): ['11-1011', '11-1021', '11-1031', '11-2011', '11-2021', '11-2022', '11-2032', '11-2033', '11-3012', '11-3013', '11-3021', '11-3031', '11-3051', '11-3061', '11-3071', '11-3111', '11-3121', '11-3131', '11-9013', '11-9021']\n",
      "\n",
      "Exact matches for '15-1299': 533\n",
      "\n",
      "=== LEVEL4 VALUES FOR SOC 15-1299 ===\n",
      "Level4 statistics:\n",
      "count    533.000000\n",
      "mean      52.837655\n",
      "std        9.812482\n",
      "min       29.090000\n",
      "25%       46.720000\n",
      "50%       51.570000\n",
      "75%       57.860000\n",
      "max      146.150000\n",
      "Name: Level4, dtype: float64\n",
      "\n",
      "Sample Level4 values:\n",
      "       Area  Level4\n",
      "43    10180   51.36\n",
      "494   10420   54.14\n",
      "904   10500   49.24\n",
      "1125  10540   49.61\n",
      "1384  10580   53.93\n",
      "1896  10740   57.83\n",
      "2314  10780   52.19\n",
      "2565  10900   56.48\n",
      "2986  11020   51.43\n",
      "3228  11100   58.79\n",
      "\n",
      "Rows where salary (64.9) >= Level4: 478 out of 533\n"
     ]
    }
   ],
   "source": [
    "# --- DIAGNOSTIC: Understand why we're getting 0 results ---\n",
    "print(\"=== DATA DIAGNOSTICS ===\\n\")\n",
    "\n",
    "# Check column names\n",
    "print(\"Table 1 columns:\", t1.columns.tolist())\n",
    "print(\"\\nTable 1 shape:\", t1.shape)\n",
    "print(\"\\nFirst few rows of Table 1:\")\n",
    "print(t1.head())\n",
    "\n",
    "# Check SOC code matching\n",
    "if \"SocCode\" in t1.columns:\n",
    "    print(\"\\n=== SOC CODE ANALYSIS ===\")\n",
    "    print(f\"\\nLooking for SOC code: '{SOCCODE_FILTER}'\")\n",
    "    print(f\"Unique SOC codes (first 20): {sorted(t1['SocCode'].unique())[:20]}\")\n",
    "    \n",
    "    # Check exact match\n",
    "    exact_match = (t1[\"SocCode\"] == SOCCODE_FILTER).sum()\n",
    "    print(f\"\\nExact matches for '{SOCCODE_FILTER}': {exact_match}\")\n",
    "    \n",
    "    # Check if there are similar codes (maybe with spaces or different format)\n",
    "    if exact_match == 0:\n",
    "        similar = t1[t1[\"SocCode\"].astype(str).str.contains(\"15-1299\", na=False)][\"SocCode\"].unique()\n",
    "        print(f\"Similar codes containing '15-1299': {similar}\")\n",
    "    \n",
    "    # If we have matches, check Level4 values\n",
    "    if exact_match > 0:\n",
    "        mask_soc = t1[\"SocCode\"] == SOCCODE_FILTER\n",
    "        print(f\"\\n=== LEVEL4 VALUES FOR SOC {SOCCODE_FILTER} ===\")\n",
    "        print(f\"Level4 statistics:\")\n",
    "        print(t1.loc[mask_soc, \"Level4\"].describe())\n",
    "        print(f\"\\nSample Level4 values:\")\n",
    "        print(t1.loc[mask_soc, [\"Area\", \"Level4\"]].head(10))\n",
    "        \n",
    "        # Check how many meet the salary requirement\n",
    "        level4_mask = SALARY >= t1.loc[mask_soc, \"Level4\"]\n",
    "        print(f\"\\nRows where salary ({SALARY}) >= Level4: {level4_mask.sum()} out of {exact_match}\")\n",
    "else:\n",
    "    print(\"ERROR: 'SocCode' column not found in Table 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f45d9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully exported 4175 eligible locations to: OFLC_Wages_2024-25_eligible_locations.xlsx\n",
      "ðŸ“ File saved at: /Users/aryamihirs/projects/product/loc-data-analysis/OFLC_Wages_2024-25_eligible_locations.xlsx\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   - Total eligible locations: 4175\n",
      "   - SOC Code: 15-1299\n",
      "   - Salary: $64.9\n",
      "   - Filter: Salary >= Level4\n"
     ]
    }
   ],
   "source": [
    "# --- EXPORT TO EXCEL ---\n",
    "import os\n",
    "# Dynamically extract parent folder name from input file path and use it as filename prefix\n",
    "parent_folder_name = os.path.basename(os.path.dirname(TABLE1_PATH))\n",
    "output_file = f\"{parent_folder_name}_eligible_locations.xlsx\"\n",
    "\n",
    "# Export final_table to Excel\n",
    "try:\n",
    "    final_table.to_excel(output_file, index=False, engine='openpyxl')\n",
    "    abs_path = os.path.abspath(output_file)\n",
    "    print(f\"âœ… Successfully exported {len(final_table)} eligible locations to: {output_file}\")\n",
    "    print(f\"ðŸ“ File saved at: {abs_path}\")\n",
    "except ImportError:\n",
    "    # If openpyxl is not installed, try with xlsxwriter or install it\n",
    "    try:\n",
    "        final_table.to_excel(output_file, index=False, engine='xlsxwriter')\n",
    "        abs_path = os.path.abspath(output_file)\n",
    "        print(f\"âœ… Successfully exported {len(final_table)} eligible locations to: {output_file}\")\n",
    "        print(f\"ðŸ“ File saved at: {abs_path}\")\n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  Need to install openpyxl or xlsxwriter to export to Excel.\")\n",
    "        print(\"Run: pip install openpyxl\")\n",
    "        # Fallback to CSV\n",
    "        csv_file = f\"{parent_folder_name}_eligible_locations.csv\"\n",
    "        final_table.to_csv(csv_file, index=False)\n",
    "        print(f\"ðŸ“„ Exported to CSV instead: {csv_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error exporting to Excel: {e}\")\n",
    "    # Fallback to CSV\n",
    "    csv_file = f\"{parent_folder_name}_eligible_locations.csv\"\n",
    "    final_table.to_csv(csv_file, index=False)\n",
    "    print(f\"ðŸ“„ Exported to CSV instead: {csv_file}\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   - Total eligible locations: {len(final_table)}\")\n",
    "print(f\"   - SOC Code: {SOCCODE_FILTER}\")\n",
    "print(f\"   - Salary: ${SALARY}\")\n",
    "print(f\"   - Filter: Salary >= Level4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
